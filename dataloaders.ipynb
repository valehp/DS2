{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LibrerÃ­as necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, SVHN, USPS\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Dataset Loader ------------------------\n",
    "def mnist_dataloader(to_rgb=True, train=True, batch_size=64):\n",
    "    trans = transforms.Compose([\n",
    "        #PIL.Image.convert('RGB'),\n",
    "        transforms.Resize(size=(32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)), \n",
    "    ])\n",
    "    mnist_dataset = MNIST('./data/MNIST', download=True, transform=trans)\n",
    "    loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def svhn_dataloader(train=True, batch_size=64):\n",
    "    trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
    "    ])\n",
    "    dataset = SVHN('./data/SVHN', download=True, transform=trans, split=('train' if train else 'test'))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def usps_loader(train=True, batch_size=64):\n",
    "    trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5), (0.5,0.5))\n",
    "    ])\n",
    "    dataset = USPS('./data/USPS', download=True, transform=trans, train=train)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Functions for F-CADA --------------------- #\n",
    "def sample_data(mnist=True, svhn=False, usps=False):\n",
    "    if mnist: dataset = mnist_dataloader()\n",
    "    elif svhn: dataset = svhn_dataloader()\n",
    "    elif usps: dataset = usps_loader()\n",
    "    n=len(dataset)\n",
    "\n",
    "    X=torch.Tensor(n,1,28,28)\n",
    "    Y=torch.LongTensor(n)\n",
    "\n",
    "    inds=torch.randperm(len(dataset))\n",
    "    for i,index in enumerate(inds):\n",
    "        x,y=dataset[index]\n",
    "        X[i]=x\n",
    "        Y[i]=y\n",
    "    return X,Y\n",
    "\n",
    "def create_target_samples(n=1, mnist=False, svhn=True, usps=False):\n",
    "    if mnist: dataset = mnist_dataloader()\n",
    "    elif svhn: dataset = svhn_dataloader()\n",
    "    elif usps: dataset = usps_loader()\n",
    "    X,Y=[],[]\n",
    "    classes=10*[n]\n",
    "\n",
    "    i=0\n",
    "    while True:\n",
    "        if len(X)==n*10:\n",
    "            break\n",
    "        x,y=dataset[i]\n",
    "        if classes[y]>0:\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            classes[y]-=1\n",
    "        i+=1\n",
    "\n",
    "    assert (len(X)==n*10)\n",
    "    return torch.stack(X,dim=0),torch.from_numpy(np.array(Y))\n",
    "\n",
    "\"\"\"\n",
    "G1: a pair of pic comes from same domain ,same class\n",
    "G3: a pair of pic comes from same domain, different classes\n",
    "\n",
    "G2: a pair of pic comes from different domain,same class\n",
    "G4: a pair of pic comes from different domain, different classes\n",
    "\"\"\"\n",
    "def create_groups(X_s,Y_s,X_t,Y_t,seed=1):\n",
    "    #change seed so every time wo get group data will different in source domain,but in target domain, data not change\n",
    "    torch.manual_seed(1 + seed)\n",
    "    torch.cuda.manual_seed(1 + seed)\n",
    "    n=X_t.shape[0] #10*shot\n",
    "    #shuffle order\n",
    "    classes = torch.unique(Y_t)\n",
    "    classes=classes[torch.randperm(len(classes))]\n",
    "    class_num=classes.shape[0]\n",
    "    shot=n//class_num\n",
    "\n",
    "    def s_idxs(c):\n",
    "        idx=torch.nonzero(Y_s.eq(int(c)))\n",
    "        return idx[torch.randperm(len(idx))][:shot*2].squeeze()\n",
    "    def t_idxs(c):\n",
    "        return torch.nonzero(Y_t.eq(int(c)))[:shot].squeeze()\n",
    "    \n",
    "    source_idxs = list(map(s_idxs, classes))\n",
    "    target_idxs = list(map(t_idxs, classes))\n",
    "\n",
    "    source_matrix=torch.stack(source_idxs)\n",
    "    target_matrix=torch.stack(target_idxs)\n",
    "\n",
    "    G1, G2, G3, G4 = [], [] , [] , []\n",
    "    Y1, Y2 , Y3 , Y4 = [], [] ,[] ,[]\n",
    "    for i in range(10):\n",
    "        for j in range(shot):\n",
    "            G1.append((X_s[source_matrix[i][j*2]],X_s[source_matrix[i][j*2+1]]))\n",
    "            Y1.append((Y_s[source_matrix[i][j*2]],Y_s[source_matrix[i][j*2+1]]))\n",
    "            G2.append((X_s[source_matrix[i][j]],X_t[target_matrix[i][j]]))\n",
    "            Y2.append((Y_s[source_matrix[i][j]],Y_t[target_matrix[i][j]]))\n",
    "            G3.append((X_s[source_matrix[i%10][j]],X_s[source_matrix[(i+1)%10][j]]))\n",
    "            Y3.append((Y_s[source_matrix[i % 10][j]], Y_s[source_matrix[(i + 1) % 10][j]]))\n",
    "            G4.append((X_s[source_matrix[i%10][j]],X_t[target_matrix[(i+1)%10][j]]))\n",
    "            Y4.append((Y_s[source_matrix[i % 10][j]], Y_t[target_matrix[(i + 1) % 10][j]]))\n",
    "\n",
    "    groups=[G1,G2,G3,G4]\n",
    "    groups_y=[Y1,Y2,Y3,Y4]\n",
    "    #make sure we sampled enough samples\n",
    "    for g in groups:\n",
    "        assert(len(g)==n)\n",
    "    return groups,groups_y\n",
    "\n",
    "def sample_groups(X_s,Y_s,X_t,Y_t,seed=1):\n",
    "\n",
    "\n",
    "    print(\"Sampling groups\")\n",
    "    return create_groups(X_s,Y_s,X_t,Y_t,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-acdec652788f>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-acdec652788f>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    print 'Creating pairs for repetition: '+str(cc)+' and sample_per_class: '+str(sample_per_class)\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Functions for CCSA ------------------------------ #\n",
    "import random\n",
    "import os\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Lambda, Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Nadam\n",
    "from keras import backend as K\n",
    "import numpy   as np\n",
    "import sys\n",
    "\n",
    "def printn(string):\n",
    "    sys.stdout.write(string)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# Re configurar create_pairs para aceptar todos los datasets -> transformar todo para pytorch??\n",
    "def Create_Pairs(domain_adaptation_task,repetition,sample_per_class):\n",
    "    UM  = domain_adaptation_task\n",
    "    cc  = repetition\n",
    "    SpC = sample_per_class\n",
    "\n",
    "    if UM != 'MNIST_to_USPS':\n",
    "        if UM != 'USPS_to_MNIST':\n",
    "            raise Exception('domain_adaptation_task should be either MNIST_to_USPS or USPS_to_MNIST')\n",
    "    if cc <0 or cc>10:\n",
    "        raise Exception('number of repetition should be between 0 and 9.')\n",
    "    if SpC <1 or SpC>7:\n",
    "            raise Exception('number of sample_per_class should be between 1 and 7.')\n",
    "\n",
    "    print 'Creating pairs for repetition: '+str(cc)+' and sample_per_class: '+str(sample_per_class)\n",
    "    X_train_target=np.load('./row_data/' + UM + '_X_train_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
    "    y_train_target=np.load('./row_data/' + UM + '_y_train_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
    "    X_train_source=np.load('./row_data/' + UM + '_X_train_source_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
    "    y_train_source=np.load('./row_data/' + UM + '_y_train_source_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
    "\n",
    "    Training_P=[]\n",
    "    Training_N=[]\n",
    "    for trs in range(len(y_train_source)):\n",
    "        for trt in range(len(y_train_target)):\n",
    "            if y_train_source[trs]==y_train_target[trt]:\n",
    "                Training_P.append([trs,trt])\n",
    "            else:\n",
    "                Training_N.append([trs,trt])\n",
    "\n",
    "    random.shuffle(Training_N)\n",
    "    Training = Training_P+Training_N[:3*len(Training_P)]\n",
    "    random.shuffle(Training)\n",
    "\n",
    "\n",
    "    X1=np.zeros([len(Training),16,16],dtype='float32')\n",
    "    X2=np.zeros([len(Training),16,16],dtype='float32')\n",
    "\n",
    "    y1=np.zeros([len(Training)])\n",
    "    y2=np.zeros([len(Training)])\n",
    "    yc=np.zeros([len(Training)])\n",
    "\n",
    "    for i in range(len(Training)):\n",
    "        in1,in2=Training[i]\n",
    "        X1[i,:,:]=X_train_source[in1,:,:]\n",
    "        X2[i,:,:]=X_train_target[in2,:,:]\n",
    "\n",
    "        y1[i]=y_train_source[in1]\n",
    "        y2[i]=y_train_target[in2]\n",
    "        if y_train_source[in1]==y_train_target[in2]:\n",
    "            yc[i]=1\n",
    "\n",
    "    if not os.path.exists('./pairs'):\n",
    "        os.makedirs('./pairs')\n",
    "\n",
    "    np.save('./pairs/' + UM + '_X1_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy', X1)\n",
    "    np.save('./pairs/' + UM + '_X2_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy', X2)\n",
    "\n",
    "    np.save('./pairs/' + UM + '_y1_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy', y1)\n",
    "    np.save('./pairs/' + UM + '_y2_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy', y2)\n",
    "    np.save('./pairs/' + UM + '_yc_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy', yc)\n",
    "    \n",
    "def Create_Model():\n",
    "    img_rows, img_cols = 16, 16\n",
    "    nb_filters = 32\n",
    "    pool_size = (2, 2)\n",
    "    kernel_size = (3, 3)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    # Model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                            padding ='valid',\n",
    "                            input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1])))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.25))\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(84))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    eps = 1e-08\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), eps))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def training_the_model(model,domain_adaptation_task,repetition,sample_per_class):\n",
    "    nb_classes=10\n",
    "    UM = domain_adaptation_task\n",
    "    cc = repetition\n",
    "    SpC = sample_per_class\n",
    "\n",
    "    if UM != 'MNIST_to_USPS':\n",
    "        if UM != 'USPS_to_MNIST':\n",
    "            raise Exception('domain_adaptation_task should be either MNIST_to_USPS or USPS_to_MNIST')\n",
    "    if cc < 0 or cc > 10:\n",
    "        raise Exception('number of repetition should be between 0 and 9.')\n",
    "    if SpC < 1 or SpC > 7:\n",
    "        raise Exception('number of sample_per_class should be between 1 and 7.')\n",
    "    epoch = 80  # Epoch number\n",
    "    batch_size = 256\n",
    "    X_test = np.load('./row_data/' + UM + '_X_test_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC)+'.npy')\n",
    "    y_test = np.load('./row_data/' + UM + '_y_test_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC)+'.npy')\n",
    "    X_test = X_test.reshape(X_test.shape[0], 16, 16, 1)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    X1 = np.load('./pairs/' + UM + '_X1_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy')\n",
    "    X2 = np.load('./pairs/' + UM + '_X2_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy')\n",
    "\n",
    "    X1 = X1.reshape(X1.shape[0], 16, 16, 1)\n",
    "    X2 = X2.reshape(X2.shape[0], 16, 16, 1)\n",
    "\n",
    "    y1 = np.load('./pairs/' + UM + '_y1_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy')\n",
    "    y2 = np.load('./pairs/' + UM + '_y2_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy')\n",
    "    yc = np.load('./pairs/' + UM + '_yc_count_' + str(cc) + '_SpC_' + str(SpC) + '.npy')\n",
    "\n",
    "    y1 = np_utils.to_categorical(y1, nb_classes)\n",
    "    y2 = np_utils.to_categorical(y2, nb_classes)\n",
    "\n",
    "    print('Training the model - Epoch '+str(epoch))\n",
    "    nn=batch_size\n",
    "    best_Acc = 0\n",
    "    for e in range(epoch):\n",
    "        if e % 10 == 0:\n",
    "            printn(str(e) + '->')\n",
    "        for i in range(len(y2) / nn):\n",
    "            loss = model.train_on_batch([X1[i * nn:(i + 1) * nn, :, :, :], X2[i * nn:(i + 1) * nn, :, :, :]],\n",
    "                                        [y1[i * nn:(i + 1) * nn, :], yc[i * nn:(i + 1) * nn, ]])\n",
    "            loss = model.train_on_batch([X2[i * nn:(i + 1) * nn, :, :, :], X1[i * nn:(i + 1) * nn, :, :, :]],\n",
    "                                        [y2[i * nn:(i + 1) * nn, :], yc[i * nn:(i + 1) * nn, ]])\n",
    "\n",
    "        Out = model.predict([X_test, X_test])\n",
    "        Acc_v = np.argmax(Out[0], axis=1) - np.argmax(y_test, axis=1)\n",
    "        Acc = (len(Acc_v) - np.count_nonzero(Acc_v) + .0000001) / len(Acc_v)\n",
    "\n",
    "        if best_Acc < Acc:\n",
    "            best_Acc = Acc\n",
    "    print str(e)\n",
    "    return best_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Functions for MME* -------------------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/SVHN\\train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "mnist = MNIST_dataloader(True, True)\n",
    "svhn = svhn_dataloader()\n",
    "#usps = usps_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 32, 32] doesn't match the broadcast shape [2, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-22657103d156>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 32, 32] doesn't match the broadcast shape [2, 32, 32]"
     ]
    }
   ],
   "source": [
    "for a in mnist:\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
