{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D38qnVtrr8GD"
   },
   "outputs": [],
   "source": [
    "# ------------------------ util --------------------------------- #\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    return (torch.max(y_pred, 1)[1] == y).float().mean().item()\n",
    "\n",
    "''' Returns the mean accuracy on the test set, given a model '''\n",
    "def eval_on_test(test_dataloader, model_fn):\n",
    "    acc = 0\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        acc += accuracy(model_fn(x), y)\n",
    "    return round(acc / float(len(test_dataloader)), 3)\n",
    "\n",
    "''' Converts a list of (x, x) pairs into two Tensors ''' \n",
    "def into_tensor(data, into_vars=True):\n",
    "    X1 = [x[0] for x in data]\n",
    "    X2 = [x[1] for x in data]\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(torch.stack(X1)).cuda(), Variable(torch.stack(X2)).cuda()\n",
    "    return Variable(torch.stack(X1)), Variable(torch.stack(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vw_mUAs3y0z2"
   },
   "outputs": [],
   "source": [
    "# ------------------------- data -------------------------------- #\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle, gzip\n",
    "\n",
    "class USPS(Dataset): # path -> where the data is\n",
    "    def __init__(self, path='/content/drive/My Drive/DS2/usps_28x28.pkl', transform=None, train=True, all=False):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        x_train, y_train, x_test, y_test = [], [], [], []\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "             (x_train, y_train), (x_test, y_test) = pickle.load(f, encoding='bytes')\n",
    "        x = np.concatenate([x_train, x_test])\n",
    "        y = np.concatenate([y_train, y_test], axis=0)\n",
    "        self.data = x\n",
    "        self.labels = y\n",
    "        self.train_imgs = x_train\n",
    "        self.train_labels = y_train\n",
    "        self.test_imgs = x_test\n",
    "        self.test_labels = y_test\n",
    "        self.len_train = len(y_train)\n",
    "        self.len_test = len(y_test)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        if all: img, label = self.data[i], self.labels[i]\n",
    "        elif not all and self.train: img, label = self.train_imgs[i], self.train_labels[i]\n",
    "        elif not all and not self.train: img, label = self.test_imgs[i], self.test_labels[i]\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, label\n",
    "# Returns dataloader or the dataset\n",
    "def usps_dataloader(batch_size=256, train=True, cuda=False, n=1800, all=True, return_loader=False):\n",
    "    trans = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(size=(16,16)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset = USPS(transform=trans, train=train, all=all)  \n",
    "    if all and return_loader: return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    if not all and return_loader: return DataLoader(dataset=dataset[:n], batch_size=batch_size, shuffle=True)\n",
    "    if not all:\n",
    "        X = torch.FloatTensor(n, 1, 16, 16)\n",
    "        Y = torch.LongTensor(n)\n",
    "        inds = torch.randperm(len(dataset))[:n]\n",
    "    else:\n",
    "        X = torch.FloatTensor(len(dataset), 1, 16, 16)\n",
    "        Y = torch.LongTensor(len(dataset))\n",
    "        inds = torch.randperm(len(dataset))\n",
    "    for i, index in enumerate(inds):\n",
    "        x, y = dataset[index]\n",
    "        X[i] = x\n",
    "        Y[i] = y\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "''' Returns the MNIST dataloader '''\n",
    "def mnist_dataloader(batch_size=256, train=True, cuda=False):\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Resize(size=(16,16)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = torchvision.datasets.MNIST('/content/drive/My Drive/ds2/data', download=True, train=train, transform=trans)\n",
    "    return torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=2, pin_memory=cuda)\n",
    "\n",
    "\n",
    "''' Returns the SVHN dataloader '''\n",
    "def svhn_dataloader(batch_size=256, train=True, cuda=False):\n",
    "    \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((16, 16)),\n",
    "        torchvision.transforms.Grayscale(),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = torchvision.datasets.SVHN('/content/drive/My Drive/ds2/data', download=True, split=('train' if train else 'test'), transform=transform)\n",
    "    return torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=2, pin_memory=cuda)\n",
    "\n",
    "\n",
    "''' Samples a subset from source into memory '''\n",
    "def sample_data(n=2000, all=True, data='mnist'):\n",
    "    if data == 'mnist':\n",
    "        transf = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(size=(16,16)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = torchvision.datasets.MNIST('/content/drive/My Drive/ds2/data', download=True, train=True, transform=transf)\n",
    "    elif data == 'svhn': \n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((16, 16)),\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = torchvision.datasets.SVHN('/content/drive/My Drive/ds2/data', download=True, split=('train' if train else 'test'), transform=transform)\n",
    "    #elif data == 'usps':\n",
    "    if all: \n",
    "        inds = torch.randperm(len(dataset))\n",
    "        X = torch.FloatTensor(len(dataset), 1, 16, 16)\n",
    "        Y = torch.LongTensor(len(dataset))\n",
    "    if not all:\n",
    "        X = torch.FloatTensor(n, 1, 16, 16)\n",
    "        Y = torch.LongTensor(n)\n",
    "        inds = torch.randperm(len(dataset))[:n]\n",
    "    for i, index in enumerate(inds):\n",
    "        x, y = dataset[index]\n",
    "        X[i] = x\n",
    "        Y[i] = y\n",
    "    return X, Y\n",
    "\n",
    "    #create a sample data for others experiments (3 to 6)\n",
    "\n",
    "\n",
    "''' Returns a subset of the target domain such that it has n_target_samples per class '''\n",
    "def create_target_samples(n=1, target='svhn'):\n",
    "    if target == 'svhn':\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((16, 16)),\n",
    "            torchvision.transforms.Grayscale(),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = torchvision.datasets.SVHN('/content/drive/My Drive/ds2/data', download=True, split='train', transform=transform)\n",
    "    elif target == 'mnist':\n",
    "        transf = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(size=(16,16)),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = torchvision.datasets.MNIST('/content/drive/My Drive/ds2/data', download=True, train=True, transform=transf)\n",
    "    elif target == 'usps':\n",
    "        trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(size=(16,16)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        dataset = USPS(transform=trans, train=True, all=False)\n",
    "    print(target, len(dataset), len(dataset[0]))\n",
    "    X, Y = [], []\n",
    "    classes = 10 * [n]\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        if len(X) == n*10:\n",
    "            break\n",
    "        x, y = dataset[i]\n",
    "        if classes[y] > 0:\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            classes[y] -= 1\n",
    "        i += 1\n",
    "\n",
    "    assert(len(X) == n*10)\n",
    "    return torch.stack(X), torch.from_numpy(np.array(Y))\n",
    "\n",
    "''' \n",
    "    Samples uniformly groups G1 and G3 from D_s x D_s and groups G2 and G4 from D_s x D_t  \n",
    "'''\n",
    "def create_groups(X_s, Y_s, X_t, Y_t, seed=1):\n",
    "    #change seed so every time wo get group data will different in source domain,but in target domain, data not change\n",
    "    torch.manual_seed(1 + seed)\n",
    "    torch.cuda.manual_seed(1 + seed)\n",
    "    n=X_t.shape[0] #10*shot\n",
    "    #shuffle order\n",
    "    classes = torch.unique(Y_t)\n",
    "    shuf_classes=classes[torch.randperm(len(classes))]\n",
    "    \n",
    "    class_num=shuf_classes.shape[0]\n",
    "    shot=n//class_num\n",
    "    #shot = n\n",
    "    \n",
    "    def get_idx(map_labels, labels):\n",
    "        for i in range(len(labels)): map_labels[labels[i].item()].append(i)\n",
    "        return map_labels\n",
    "    def g1_sample(Y, ids, X): # X^s_i, X^s_s\n",
    "        i1 = random.choice(ids)\n",
    "        i2 = random.choice(ids)\n",
    "        return (X[i1], X[i2]), (Y[i1], Y[i2])\n",
    "    def g2_sample(Ys, Yt, idx_s, idx_t, Xs, Xt): # X^s_i, X^t_i\n",
    "        s = random.choice(idx_s)\n",
    "        t = random.choice(idx_t)\n",
    "        return ( Xs[s], Xt[t] ), (Ys[s], Yt[t])\n",
    "    def g3_sample(X, Y, idx_i, idx_j): # X^s_i , X^s_j\n",
    "        i = random.choice(idx_i)\n",
    "        j = random.choice(idx_j)\n",
    "        return (X[i], X[j]), (Y[i], Y[j])\n",
    "    def g4_sample(Xs, Xt, Ys, Yt, idx_s, idx_t): # X^s_i, X^t_j\n",
    "        i = random.choice(idx_s)\n",
    "        j = random.choice(idx_t)\n",
    "        return (Xs[i], Xt[j]), (Ys[i], Yt[j])\n",
    "    \n",
    "    s_idx = torch.arange(0, len(X_s))\n",
    "    t_idx = torch.arange(0, len(X_t))\n",
    "    s_labels = np.unique(Y_s)\n",
    "    t_labels = np.unique(Y_t)\n",
    "    s_map = dict(zip(s_labels, [ [] for i in range(len(s_labels)) ]))\n",
    "    t_map = dict(zip(t_labels, [ [] for i in range(len(t_labels)) ]))\n",
    "    \n",
    "    s_map = get_idx(s_map, Y_s)\n",
    "    t_map = get_idx(t_map, Y_t)\n",
    "    G1, G2, G3, G4 = [], [], [], []\n",
    "    Y1, Y2, Y3, Y4 = [], [], [], []\n",
    "    for i in s_labels:\n",
    "        for j in t_labels:\n",
    "            tmp1 = g1_sample(Y_s, s_map[i], X_s)\n",
    "            tmp2 = g2_sample(Y_s, Y_t, s_map[i], t_map[j], X_s, X_t)\n",
    "            ii = i\n",
    "            while ii == j or ii not in s_labels: ii = random.choice(s_labels)\n",
    "            tmp3 = g3_sample(X_s, Y_s, s_map[ii], s_map[j])\n",
    "            ii = i\n",
    "            while ii == j or ii not in s_labels: ii = random.choice(s_labels)\n",
    "            tmp4 = g4_sample(X_s, X_t, Y_s, Y_t, s_map[ii], t_map[j])\n",
    "            G1.append(tmp1[0])\n",
    "            G2.append(tmp2[0])\n",
    "            G3.append(tmp3[0])\n",
    "            G4.append(tmp4[0])\n",
    "            Y1.append(tmp1[1])\n",
    "            Y2.append(tmp2[1])\n",
    "            Y3.append(tmp3[1])\n",
    "            Y4.append(tmp4[1])\n",
    "    groups = [G1, G2, G3, G4]\n",
    "    labels = [Y1, Y2, Y3, Y4]\n",
    "    print(n)\n",
    "    # Make sure we sampled enough samples\n",
    "    for g in groups:\n",
    "        #print(len(g), n)\n",
    "        assert(len(g) > n)\n",
    "    return groups, labels\n",
    "\n",
    "''' Sample groups G1, G2, G3, G4 '''\n",
    "def sample_groups(n_target_samples=2, train_mode=2, n=2000, source='mnist', target='svhn'):\n",
    "    if training_mode == 2:\n",
    "        if source == 'usps': X_s, Y_s = usps_dataloader(all=False, train=True, return_loader=False)\n",
    "        else: X_s, y_s = sample_data(all=True, data=source)\n",
    "    else:\n",
    "        if source == 'usps': X_s, Y_s = usps_dataloader(all=True, return_loader=False, n=n)\n",
    "        else: X_s, y_s = sample_data(n=n, all=False, data=source)\n",
    "    X_t, y_t = create_target_samples(n=n_target_samples, target=target)\n",
    "    \n",
    "    #print(\"Sampling groups\")\n",
    "    return create_groups(X_s, y_s, X_t, y_t), (X_s, y_s, X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfuVzo9tzyI6"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------- models ---------------------------------- #\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "'''\n",
    "    Domain-Class Discriminator (see (3) in the paper)\n",
    "    Takes in the concatenated latent representation of two samples from\n",
    "    G1, G2, G3 or G4, and outputs a class label, one of [0, 1, 2, 3]\n",
    "'''\n",
    "class DCD(nn.Module):\n",
    "    def __init__(self, H=64, D_in=256):\n",
    "        super(DCD, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, H)\n",
    "        self.out = nn.Linear(H, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return F.softmax(self.out(out), dim=1)\n",
    "\n",
    "''' Called h in the paper. Gives class predictions based on the latent representation '''\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, D_in=64):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.out = nn.Linear(D_in, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.out(x), dim=1)\n",
    "\n",
    "''' \n",
    "    Creates latent representation based on data. Called g in the paper.\n",
    "    Like in the paper, we use g_s = g_t = g, that is, we share weights between target\n",
    "    and source representations.\n",
    "    Model is as specified in section 4.1. See https://github.com/kuangliu/pytorch-cifar/blob/master/models/lenet.py\n",
    "'''\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 64)\n",
    "        self.flat  = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.flat(out)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0xULhMjEDCV"
   },
   "outputs": [],
   "source": [
    "# ---------------------------- training -------------------------------------- #\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "#from data import mnist_dataloader, svhn_dataloader\n",
    "#from models import Classifier, Encoder, DCD\n",
    "#from util import eval_on_test, into_tensor\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_fn(encoder, classifier):\n",
    "    return lambda x: classifier(encoder(x))\n",
    "\n",
    "''' Pretrain the encoder and classifier as in (a) in figure 2. '''\n",
    "def pretrain(epochs=5, batch_size=256, cuda=False, domain='svhn'):\n",
    "    print(\"Test: \", domain)\n",
    "    if domain == 'svhn':\n",
    "        train_loader = svhn_dataloader(train=True, cuda=cuda)\n",
    "        test_loader = svhn_dataloader(train=False, cuda=cuda)\n",
    "    elif domain == 'mnist':\n",
    "        train_loader = mnist_dataloader(train=True, cuda=cuda)\n",
    "        test_loader = mnist_dataloader(train=False, cuda=cuda)\n",
    "    elif domain == 'usps':\n",
    "        train_loader = usps_dataloader(train=True, cuda=cuda)\n",
    "        test_loader = usps_dataloader(train=False, cuda=cuda)\n",
    "\n",
    "    X_s, y_s, _, _ = data\n",
    "\n",
    "    classifier = Classifier()\n",
    "    encoder = Encoder()\n",
    "\n",
    "    if cuda:\n",
    "        classifier.cuda()\n",
    "        encoder.cuda()\n",
    "        device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "\n",
    "    ''' Jointly optimize both encoder and classifier ''' \n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()))\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print(\"A entrenar ... \", epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train()\n",
    "        encoder.train()\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = classifier(encoder(imgs))\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = 0\n",
    "        classifier.eval()\n",
    "        encoder.eval()\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_pred = classifier(encoder(imgs))\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            valid_loss += loss.item()\n",
    "            acc+=(torch.max(y_pred,1)[1]==labels).float().mean().item()\n",
    "        accuracy = round( acc / float(len(test_loader)), 3 )\n",
    "        #print( \"STEP1 ------- Epoch {} / {} -> \\t Accuracy: {} \\t Train loss: {} \\t Validation loss: {}\".format(epoch, epochs, accuracy, train_loss/len(train_loader), valid_loss/len(test_loader)) )\n",
    "    \n",
    "    return encoder, classifier\n",
    "\n",
    "''' Train the discriminator while the encoder is frozen '''\n",
    "def train_discriminator(encoder, groups, n_target_samples=2, cuda=False, epochs=20, source='mnist', target='svhn', training_mode=2):\n",
    "    if training_mode == 2: x_s , Y_s = sample_data(all=True, data=source)\n",
    "    else: X_s, Y_s = sample_data( n = (2000 if source=='mnist' else 1800), all=False, data=source )\n",
    "    X_t, Y_t = create_target_samples(n=n_target_samples, target=target)\n",
    "\n",
    "    if cuda: device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "\n",
    "    discriminator = DCD(H=512, D_in=128) # Takes in concatenated hidden representations\n",
    "    discriminator.to(device)\n",
    "    encoder.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    #class_optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()) )\n",
    "    # Only train DCD\n",
    "    optimizer = optim.Adam(discriminator.parameters())\n",
    "    \n",
    "    # Size of group G2, the smallest one, times the amount of groups\n",
    "    n_iters = 4 * n_target_samples\n",
    "    #print(\"\\t\\t --- Training DC ---\")\n",
    "    \n",
    "    encoder.eval()\n",
    "    discriminator.train()\n",
    "    loss_mean = []\n",
    "    for epoch in range(epochs):\n",
    "        acc = 0\n",
    "        for ii in range(n_iters):\n",
    "            optimizer.zero_grad()\n",
    "            i = ii if(ii<4) else ii%4\n",
    "            x1, x2 = groups[i][ random.randint(0, len(groups[i])-1) ]\n",
    "            x1, x2 = Variable(x1), Variable(x2)\n",
    "            truth = torch.tensor([i])\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            z_1 = encoder(x1.unsqueeze(0)) # z_i : latent sppace of x1\n",
    "            z_2 = encoder(x2.unsqueeze(0))\n",
    "            z_cat = torch.cat([z_1, z_2], dim=1)\n",
    "            z_cat = z_cat.to(device)\n",
    "            y_pred = discriminator(z_cat)\n",
    "            loss = loss_fn(y_pred, truth)\n",
    "            loss_mean.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        #print(\"STEP 2 ------- Epoch: {} / {}  ->  Loss: {}\".format(epoch, epochs, loss_mean[-1]/n_iters ) )    \n",
    "  \n",
    "    return discriminator\n",
    "\n",
    "''' FADA Loss, as given by (4) in the paper. The minus sign is shifted because it seems to be wrong '''\n",
    "def fada_loss(y_pred_g2, g1_true, y_pred_g4, g3_true, gamma=0.2):\n",
    "    return -gamma * torch.mean(g1_true * torch.log(y_pred_g2) + g3_true * torch.log(y_pred_g4))\n",
    "\n",
    "''' Step three of the algorithm, train everything except the DCD '''\n",
    "def train(encoder, discriminator, classifier, data, groups, groups_labels, n_target_samples=2, cuda=False, epochs=20, batch_size=256, plot_accuracy=False, target='svhn', source='mnist', test_n=None):\n",
    "    if target == 'mnist': test_loader = mnist_dataloader(train=False, cuda=cuda)\n",
    "    elif target == 'svhn': test_loader = svhn_dataloader(train=False, cuda=cuda)\n",
    "    elif target == 'usps':\n",
    "        if test_n: test_loader = usps_dataloader(train=False, cuda=cuda, n=test_n, return_loader=True)\n",
    "        else: test_loader = usps_dataloader(train=False, cuda=cuda, return_loader=True)\n",
    "    \n",
    "    if cuda: device = 'cuda'\n",
    "    else: device = 'cpu'\n",
    "    encoder.to(device)\n",
    "    classifier.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    X_s, Y_s, X_t, Y_t = data\n",
    "\n",
    "    G1, G2, G3, G4 = groups\n",
    "    Y1, Y2, Y3, Y4 = groups_labels\n",
    "\n",
    "    ''' Two optimizers, one for DCD (which is frozen) and one for class training ''' \n",
    "    class_optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()))\n",
    "    #class_optimizer = optim.Adam(classifier.parameters())\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "    dcd_optimizer = optim.Adam(discriminator.parameters())\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_dcd = nn.CrossEntropyLoss()\n",
    "    n_iters = 4 * n_target_samples\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        # Training g & h\n",
    "        # Set g & h training mode while DCD is frozen\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "        discriminator.eval()\n",
    "        epoch_loss = 0\n",
    "        loss_mean = []\n",
    "        gh_losses = []\n",
    "        dcd_losses = []\n",
    "        aux_loss = 0\n",
    "        for i in range(n_target_samples): # n-shot -> n labels \n",
    "            for j in range(4): # 4 groups\n",
    "            # Select samples from Gj\n",
    "                class_optimizer.zero_grad()\n",
    "                x1, x2 = groups[j][i]\n",
    "                y1, y2 = groups_labels[j][i]\n",
    "                x1, y1 = x1.type(torch.FloatTensor), y1.type(torch.LongTensor)\n",
    "                x1, y1 = x1.to(device), y1.unsqueeze(0).to(device)\n",
    "                z1 = encoder(x1.unsqueeze(0))\n",
    "                z1 = z1.to(device)\n",
    "                y_pred1 = classifier(z1)\n",
    "                loss_1 = loss_fn(y_pred1, y1)\n",
    "                aux_loss += loss_1.item()\n",
    "                loss_1.backward()\n",
    "                class_optimizer.step()\n",
    "\n",
    "                class_optimizer.zero_grad()\n",
    "                x2, y2 = x2.type(torch.FloatTensor), y2.type(torch.LongTensor)\n",
    "                x2, y2 = x2.to(device), y2.unsqueeze(0).to(device)\n",
    "                z2 = encoder(x2.unsqueeze(0))\n",
    "                z2 = z2.to(device)\n",
    "                y_pred2 = classifier(z2)\n",
    "                loss_2 = loss_fn(y_pred2, y2)\n",
    "                aux_loss += loss_2.item()\n",
    "                loss_2.backward()\n",
    "                class_optimizer.step()\n",
    "                if j == 1 or j == 3:\n",
    "                    encoder_optimizer.zero_grad()\n",
    "                    x_1, x_2 = groups[j][i]\n",
    "                    y_1, y_2 = groups_labels[j][i]\n",
    "                    x_1, y_1 = x_1.type(torch.FloatTensor), y_1.type(torch.LongTensor)\n",
    "                    x_2, y_2 = x_2.type(torch.FloatTensor), y_2.type(torch.LongTensor)\n",
    "                    x1, x2, y1, y2 = x_1.to(device), x_2.to(device), y_1.unsqueeze(0).to(device), y_2.unsqueeze(0).to(device)\n",
    "                    z1 = encoder(x1.unsqueeze(0))\n",
    "                    z2 = encoder(x2.unsqueeze(0))\n",
    "                    z1, z2 = z1.to(device), z2.to(device)\n",
    "                    zcat = torch.cat([z1, z2], dim=1).to(device)\n",
    "                    yd = discriminator(zcat)\n",
    "                    loss_d = loss_fn(yd, torch.tensor([j-1]).to(device) )\n",
    "                    aux_loss += loss_d.item()\n",
    "                    loss_d.backward()\n",
    "                    encoder_optimizer.step()\n",
    "        gh_losses.append(aux_loss/(6*n_target_samples))\n",
    "\n",
    "        # Train DCD while g&h are frozen\n",
    "        encoder.eval()\n",
    "        classifier.eval()\n",
    "        discriminator.train()\n",
    "        aux_loss = 0\n",
    "        for i in range(n_target_samples): # n-shots\n",
    "            for j in range(4): # Group\n",
    "                dcd_optimizer.zero_grad()\n",
    "                x1, x2 = groups[j][i]\n",
    "                x1, x2 = x1.type(torch.FloatTensor), x2.type(torch.FloatTensor)\n",
    "                truth = torch.tensor([j]).type(torch.LongTensor)\n",
    "                x1, x2 = Variable(x1), Variable(x2)\n",
    "                x1, x2 = x1.to(device), x2.to(device)\n",
    "                z1 = encoder(x1.unsqueeze(0))\n",
    "                z2 = encoder(x2.unsqueeze(0))\n",
    "                z_cat = torch.cat([z1,z2], dim=1).to(device)\n",
    "                g_pred = discriminator(z_cat)\n",
    "                lossd = loss_dcd(g_pred.to(device), truth.to(device))\n",
    "                aux_loss += lossd.item()\n",
    "                lossd.backward()\n",
    "                dcd_optimizer.step()\n",
    "        dcd_losses.append(aux_loss/(4*n_target_samples))\n",
    "    \n",
    "        # Testing for accuracy\n",
    "        classifier.eval()\n",
    "        encoder.eval()\n",
    "        discriminator.eval()\n",
    "        acc = 0\n",
    "        aux = 0\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_pred = classifier(encoder(imgs))\n",
    "            aux = (torch.max(y_pred,1)[1]==labels).float().mean().item()\n",
    "            #print(\"aux: \", aux, end=\"\\t\")\n",
    "            acc += aux\n",
    "        accuracies.append(acc/len(test_loader))\n",
    "\n",
    "        #print(\"STEP 3/4: Epoch: {} / {} -> \\t g & h losses: {} \\t DCD loss: {} \\t Accuracy: {}\".format(epoch, epochs, gh_losses[-1], dcd_losses[-1], accuracies[-1]) )\n",
    "\n",
    "        #acc = eval_on_test(test_dataloader, model_fn(encoder, classifier))\n",
    "        #if plot_accuracy:\n",
    "            #accuracies.append(acc)\n",
    "        # Train DCD\n",
    "    print(\"{} to {} \\t {}-shot \\t {} epochs\".format(source.upper(), target.upper(), n_target_samples, epochs) )\n",
    "    if plot_accuracy:\n",
    "        plt.plot(range(len(accuracies)), accuracies)\n",
    "        plt.title(target.upper()+\" test accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.show()\n",
    "    print(\"max accuracy: {} - mean accuracy: {} - min accuracy: {}\".format(np.max(accuracies), np.mean(accuracies), np.min(accuracies)))\n",
    "    return np.max(accuracies), np.mean(accuracies), np.min(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Rpo4TeRQz-e8",
    "outputId": "ce63e7cc-0a98-40fb-eba5-b14c6f1999c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition: 0 \t task: mnist to usps \t training mode: 1 \t 1-shot \t source samples: 2000 \t target samples: 1800\n",
      "usps 9298 2\n",
      "10\n",
      "Test:  mnist\n",
      "A entrenar ...  20\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ main ---------------------------------------- #\n",
    "#from training import pretrain, train_discriminator, train\n",
    "#from data import sample_groups\n",
    "import torch\n",
    "\n",
    "n_target_samples_init = 1\n",
    "plot_accuracy = True\n",
    "training_mode = 1\n",
    "\"\"\"\n",
    "training_mode = 1 -> take 2000 train samples from mnist and 1800 train samples of usps\n",
    "training_mode = 2 -> take all train samples and all test samples (for testing)\n",
    "\"\"\"\n",
    "data_options = ['mnist', 'svhn', 'usps']\n",
    "source = data_options[0] # M -> U\n",
    "target = data_options[2] \n",
    "source_samples = 2000\n",
    "target_samples = 1800\n",
    "max_accs, mean_accs, min_accs = [], [], []\n",
    "if __name__ == '__main__':\n",
    "    file = open('/content/drive/My Drive/DS2/resultados.txt', 'a')\n",
    "    cuda = torch.cuda.is_available()\n",
    "    for n_target_samples in range(n_target_samples_init, 3, 1):\n",
    "        max_aux , mean_aux, min_aux = [], [], []\n",
    "        for k in range(3):\n",
    "            print(\"Repetition: {} \\t task: {} to {} \\t training mode: {} \\t {}-shot \\t source samples: {} \\t target samples: {}\".format(k, source, target, training_mode, n_target_samples, source_samples, target_samples) )\n",
    "            if training_mode == 2: groups, data = sample_groups(n_target_samples=n_target_samples, train_mode=training_mode, source=source, target=target)\n",
    "            if training_mode == 1: groups, data = sample_groups(n_target_samples=n_target_samples, train_mode=training_mode, source=source, target=target, n=source_samples)  # M->U or U->M\n",
    "            labels = groups[1]\n",
    "            groups = groups[0]\n",
    "            encoder, classifier = pretrain(cuda=cuda, epochs=20, domain=source) # Train g and h on D_s ; epoch=20\n",
    "            discriminator = train_discriminator(encoder, groups, n_target_samples, False, epochs=50, source=source, target=target, training_mode=training_mode) # Train DCD sharing weights g_s = g_t = g ; epochs=50\n",
    "            max_ac, mean_ac, min_ac = train(encoder=encoder, discriminator=discriminator, classifier=classifier, data=data, groups=groups, groups_labels=labels, n_target_samples=n_target_samples, cuda=cuda, epochs=200, plot_accuracy=plot_accuracy, target=target, source=source, test_n=target_samples)\n",
    "            del encoder\n",
    "            del classifier\n",
    "            del discriminator\n",
    "            max_aux.append(max_ac)\n",
    "            mean_aux.append(mean_ac)\n",
    "            min_aux.append(min_ac)\n",
    "        max_accs.append(max_aux)\n",
    "        mean_accs.append(mean_aux)\n",
    "        min_accs.append(min_aux)\n",
    "        file.write( \"\\nTask: {} to {} \\t epochs: 300 \\t {}-shot\\n\".format(source, target, n_target_samples) )\n",
    "        file.write( \"All of max accuracys: {} \\t mean of max accs: {}\\n\".format(max_aux, np.mean(max_aux)) )\n",
    "        file.write( \"All of mean accuracys: {} \\t mean of mean accs: {}\\n\".format(mean_aux, np.mean(mean_aux)) )\n",
    "        file.write( \"All of min accuracys: {} \\t mean of min accs: {}\\n\\n\".format(min_aux, np.mean(min_aux)) )\n",
    "        print( \"All of max accuracys: {} \\t mean of max accs: {}\".format(max_aux, np.mean(max_aux)) )\n",
    "        print( \"All of mean accuracys: {} \\t mean of mean accs: {}\".format(mean_aux, np.mean(mean_aux)) )\n",
    "        print( \"All of min accuracys: {} \\t mean of min accs: {}\\n\".format(min_aux, np.mean(min_aux)) )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VphkYv2JCw1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fada.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
